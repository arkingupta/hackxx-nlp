{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://cses.ucsd.edu/assets/images/cseslogo-300.png\"> <br>\n",
    "<center> <h1> Natural Language Processing Workshop</h1> <br>\n",
    "by Arkin Gupta and Saurabh Kanhegaonkar\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> Text Pre-processing </h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading corpus\n",
    "#Brown corpus: https://en.wikipedia.org/wiki/Brown_Corpus\n",
    "\n",
    "#Method 1\n",
    "all_sentences = []\n",
    "for sentence in brown.sents():\n",
    "    all_sentences.append(sentence)\n",
    "\n",
    "#Method 2 (Better) - List comprehension\n",
    "all_sentences = [sentence for sentence in brown.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Note that these are already tokenized\n",
    "\n",
    "#How to going back-and-forth\n",
    "test_string = nltk.word_tokenize('NLP is fun')\n",
    "test_array = ' '.join(['NLP', 'is', 'fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'is', 'fun']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP is fun'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Capitalization - it is important to get rid of it, you don't want \"Hello\" and \"hello\" to be considered as different words by any model\n",
    "'TEST'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert arrays to strings\n",
    "new_sents = [' '.join(sentence) for sentence in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make all strings lower case\n",
    "new_sents = [sentence.lower() for sentence in new_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making a large corpus - makes a lot of things easier\n",
    "large_corpus = ''\n",
    "for sentence in all_sentences:\n",
    "    string_version = ' '.join(sentence)\n",
    "    large_corpus = large_corpus + ' ' + (string_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . The jury further said in term-end presentments that the City Executive Committee , which h\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_corpus[0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Capitalization\n",
    "large_corpus = large_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place . the jury further said in term-end presentments that the city executive committee , which h\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_corpus[0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "large_corpus = re.sub(r'[^a-zA-Z0-9 .]', '', large_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the fulton county grand jury said friday an investigation of atlantas recent primary election produced  no evidence  that any irregularities took place . the jury further said in termend presentments that the city executive committee  which had over'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_corpus[0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preserving full-stops helps, since you can now split by them\n",
    "split_by_fullstop = (large_corpus).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "#large_corpus_tokens = nltk.word_tokenize(large_corpus)\n",
    "large_corpus_tokens = large_corpus.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'the',\n",
       " 'fulton',\n",
       " 'county',\n",
       " 'grand',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " 'atlantas',\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '',\n",
       " 'no',\n",
       " 'evidence',\n",
       " '',\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.',\n",
       " 'the',\n",
       " 'jury',\n",
       " 'further',\n",
       " 'said',\n",
       " 'in',\n",
       " 'termend',\n",
       " 'presentments',\n",
       " 'that',\n",
       " 'the',\n",
       " 'city',\n",
       " 'executive',\n",
       " 'committee',\n",
       " '',\n",
       " 'which',\n",
       " 'had',\n",
       " 'overall',\n",
       " 'charge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'election']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_corpus_tokens[0:46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing stop words - crucial step\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "final_tokens = []\n",
    "for w in large_corpus_tokens: \n",
    "    if w not in stop_words: \n",
    "        if w != '':\n",
    "            final_tokens.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fulton',\n",
       " 'county',\n",
       " 'grand',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'friday',\n",
       " 'investigation',\n",
       " 'atlantas',\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " 'evidence',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'termend']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tokens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These are the basic steps you need to do before working with any corpus! \n",
    "#We have a nice list of words and full-stops which we can work with.\n",
    "#Now let's look at some more advanced techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization\n",
    "<img src = \"https://pythonspot-9329.kxcdn.com/wp-content/uploads/2016/08/word-stem.png.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golf\n",
      "golf\n",
      "became a golf\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "words = ['golf', \"golfing\", \"became a golfer\"]\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golf\n",
      "i went golfing\n",
      "became a golfer\n"
     ]
    }
   ],
   "source": [
    "words = ['golf', \"i went golfing\", \"became a golfer\"]\n",
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"golfing\", 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Adding stemming to our corpus\n",
    "stemmed_tokens = [ps.stem(word) for word in final_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2>Sentence Embeddings </h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#What are embeddings? Why do we need them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Embeddings (Documents)\n",
    "<img src = \"https://www.researchgate.net/profile/Heloisa_Rocha/publication/221228354/figure/fig2/AS:650816818003985@1532178229971/TF-IDF-formula-2.png/\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_sentences = ' '.join(final_tokens).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fulton county grand jury said friday investigation atlantas recent primary election produced evidence irregularities took place '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' jury said termend presentments city executive committee overall charge election deserves praise thanks city atlanta manner election conducted '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_num_sentences = len(final_sentences)\n",
    "\n",
    "#Calculate idf for word\n",
    "def idf(word):\n",
    "    num_sentences_word_found = 0\n",
    "    for sentence in final_sentences:\n",
    "        if word in sentence:\n",
    "            num_sentences_word_found +=1 \n",
    "    \n",
    "    if(num_sentences_word_found == 0):\n",
    "        return 1\n",
    "    #Return idf\n",
    "    return math.log10(total_num_sentences/num_sentences_word_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1.0125974010372412\n",
      "banana 4.045940724172465\n"
     ]
    }
   ],
   "source": [
    "idf_dict = {}\n",
    "for word in ('the',  'banana'):\n",
    "    idf_dict[word] = idf(word)\n",
    "    print(word, idf_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = nltk.FreqDist(final_tokens).most_common(500)\n",
    "top_words = [x for (x,y) in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'one',\n",
       " 'would',\n",
       " 'said',\n",
       " 'new',\n",
       " 'could',\n",
       " 'time',\n",
       " 'two',\n",
       " 'may',\n",
       " 'first',\n",
       " 'like',\n",
       " 'man',\n",
       " 'even',\n",
       " 'made',\n",
       " 'also',\n",
       " 'many',\n",
       " 'must',\n",
       " 'years',\n",
       " 'af',\n",
       " 'back',\n",
       " 'well',\n",
       " 'much',\n",
       " 'way',\n",
       " 'people',\n",
       " 'mr.',\n",
       " 'little',\n",
       " 'state',\n",
       " 'good',\n",
       " 'make',\n",
       " 'world',\n",
       " 'still',\n",
       " 'see',\n",
       " 'men',\n",
       " 'work',\n",
       " 'long',\n",
       " 'get',\n",
       " 'life',\n",
       " 'never',\n",
       " 'day',\n",
       " 'another',\n",
       " 'know',\n",
       " 'last',\n",
       " 'us',\n",
       " 'might',\n",
       " 'great',\n",
       " 'old',\n",
       " 'year',\n",
       " 'states',\n",
       " 'come',\n",
       " 'since',\n",
       " 'go',\n",
       " 'came',\n",
       " 'right',\n",
       " 'used',\n",
       " 'take',\n",
       " 'three',\n",
       " 'house',\n",
       " 'use',\n",
       " 'without',\n",
       " 'place',\n",
       " 'american',\n",
       " 'around',\n",
       " 'however',\n",
       " 'home',\n",
       " '1',\n",
       " 'small',\n",
       " 'found',\n",
       " 'mrs.',\n",
       " 'thought',\n",
       " 'went',\n",
       " 'say',\n",
       " 'part',\n",
       " 'general',\n",
       " 'high',\n",
       " 'upon',\n",
       " 'school',\n",
       " 'every',\n",
       " 'dont',\n",
       " 'got',\n",
       " 'united',\n",
       " 'left',\n",
       " 'number',\n",
       " '2',\n",
       " 'course',\n",
       " 'war',\n",
       " 'always',\n",
       " 'away',\n",
       " 'something',\n",
       " 'fact',\n",
       " 'water',\n",
       " 'though',\n",
       " 'public',\n",
       " 'less',\n",
       " 'put',\n",
       " 'think',\n",
       " 'almost',\n",
       " 'hand',\n",
       " 'enough',\n",
       " 'took',\n",
       " 'far',\n",
       " 'head',\n",
       " 'yet',\n",
       " 'government',\n",
       " 'system',\n",
       " 'set',\n",
       " 'better',\n",
       " 'told',\n",
       " 'night',\n",
       " 'nothing',\n",
       " 'end',\n",
       " 'didnt',\n",
       " 'days',\n",
       " 'called',\n",
       " 'eyes',\n",
       " 'find',\n",
       " 'going',\n",
       " 'look',\n",
       " 'asked',\n",
       " 'later',\n",
       " 'knew',\n",
       " 'point',\n",
       " 'next',\n",
       " 'program',\n",
       " 'city',\n",
       " 'business',\n",
       " 'group',\n",
       " 'give',\n",
       " 'toward',\n",
       " 'young',\n",
       " 'let',\n",
       " 'room',\n",
       " 'president',\n",
       " 'side',\n",
       " 'social',\n",
       " 'present',\n",
       " 'given',\n",
       " 'several',\n",
       " 'order',\n",
       " 'national',\n",
       " 'second',\n",
       " 'possible',\n",
       " 'rather',\n",
       " 'face',\n",
       " 'per',\n",
       " 'among',\n",
       " 'form',\n",
       " 'often',\n",
       " 'important',\n",
       " 'things',\n",
       " 'looked',\n",
       " 'early',\n",
       " 'white',\n",
       " 'john',\n",
       " 'case',\n",
       " 'large',\n",
       " 'four',\n",
       " 'need',\n",
       " 'big',\n",
       " 'become',\n",
       " 'within',\n",
       " 'felt',\n",
       " 'children',\n",
       " 'along',\n",
       " 'saw',\n",
       " 'best',\n",
       " 'church',\n",
       " 'ever',\n",
       " 'least',\n",
       " 'power',\n",
       " 'development',\n",
       " 'others',\n",
       " 'seemed',\n",
       " 'thing',\n",
       " 'light',\n",
       " 'family',\n",
       " 'interest',\n",
       " 'want',\n",
       " 'members',\n",
       " 'mind',\n",
       " 'area',\n",
       " 'country',\n",
       " 'although',\n",
       " 'turned',\n",
       " 'done',\n",
       " 'open',\n",
       " 'god',\n",
       " 'service',\n",
       " 'problem',\n",
       " 'certain',\n",
       " 'kind',\n",
       " 'different',\n",
       " 'thus',\n",
       " 'began',\n",
       " 'door',\n",
       " 'help',\n",
       " 'sense',\n",
       " 'means',\n",
       " 'whole',\n",
       " 'matter',\n",
       " 'perhaps',\n",
       " 'times',\n",
       " 'york',\n",
       " 'law',\n",
       " 'human',\n",
       " 'line',\n",
       " '3',\n",
       " 'name',\n",
       " 'example',\n",
       " 'action',\n",
       " 'company',\n",
       " 'hands',\n",
       " 'local',\n",
       " 'today',\n",
       " 'show',\n",
       " 'whether',\n",
       " 'five',\n",
       " 'history',\n",
       " 'gave',\n",
       " 'either',\n",
       " 'act',\n",
       " 'feet',\n",
       " 'across',\n",
       " 'taken',\n",
       " 'past',\n",
       " 'quite',\n",
       " 'anything',\n",
       " 'seen',\n",
       " 'death',\n",
       " 'experience',\n",
       " 'body',\n",
       " 'week',\n",
       " 'im',\n",
       " 'half',\n",
       " 'really',\n",
       " 'word',\n",
       " 'field',\n",
       " 'car',\n",
       " 'words',\n",
       " 'already',\n",
       " 'information',\n",
       " 'tell',\n",
       " 'shall',\n",
       " 'together',\n",
       " 'college',\n",
       " 'money',\n",
       " 'period',\n",
       " 'held',\n",
       " 'keep',\n",
       " 'sure',\n",
       " 'probably',\n",
       " 'free',\n",
       " 'seems',\n",
       " 'political',\n",
       " 'real',\n",
       " 'cannot',\n",
       " 'behind',\n",
       " 'question',\n",
       " 'air',\n",
       " 'office',\n",
       " 'making',\n",
       " 'brought',\n",
       " 'miss',\n",
       " 'whose',\n",
       " 'special',\n",
       " 'major',\n",
       " 'heard',\n",
       " 'problems',\n",
       " 'federal',\n",
       " 'became',\n",
       " 'study',\n",
       " 'ago',\n",
       " 'moment',\n",
       " 'available',\n",
       " 'known',\n",
       " 'result',\n",
       " 'street',\n",
       " 'economic',\n",
       " 'boy',\n",
       " 'position',\n",
       " 'reason',\n",
       " 'change',\n",
       " 'south',\n",
       " 'board',\n",
       " 'individual',\n",
       " 'job',\n",
       " 'areas',\n",
       " 'society',\n",
       " 'west',\n",
       " 'close',\n",
       " 'turn',\n",
       " 'community',\n",
       " 'true',\n",
       " 'love',\n",
       " 'court',\n",
       " 'force',\n",
       " 'full',\n",
       " 'cost',\n",
       " 'seem',\n",
       " 'wife',\n",
       " 'future',\n",
       " 'age',\n",
       " 'wanted',\n",
       " 'voice',\n",
       " 'department',\n",
       " 'center',\n",
       " 'woman',\n",
       " 'control',\n",
       " 'common',\n",
       " 'policy',\n",
       " 'necessary',\n",
       " 'following',\n",
       " 'front',\n",
       " 'sometimes',\n",
       " 'students',\n",
       " 'six',\n",
       " 'girl',\n",
       " 'clear',\n",
       " 'land',\n",
       " 'provide',\n",
       " 'feel',\n",
       " 'party',\n",
       " 'able',\n",
       " 'mother',\n",
       " 'music',\n",
       " '4',\n",
       " 'nations',\n",
       " 'education',\n",
       " 'university',\n",
       " 'child',\n",
       " 'effect',\n",
       " 'level',\n",
       " 'run',\n",
       " 'stood',\n",
       " 'military',\n",
       " 'town',\n",
       " 'short',\n",
       " 'morning',\n",
       " 'total',\n",
       " 'outside',\n",
       " 'rate',\n",
       " 'figure',\n",
       " 'art',\n",
       " 'class',\n",
       " 'ill',\n",
       " 'century',\n",
       " 'washington',\n",
       " 'north',\n",
       " 'usually',\n",
       " 'plan',\n",
       " 'leave',\n",
       " 'therefore',\n",
       " 'evidence',\n",
       " 'top',\n",
       " 'million',\n",
       " 'sound',\n",
       " 'black',\n",
       " 'strong',\n",
       " 'hard',\n",
       " 'tax',\n",
       " 'various',\n",
       " 'says',\n",
       " 'believe',\n",
       " 'type',\n",
       " 'value',\n",
       " 'play',\n",
       " 'surface',\n",
       " 'soon',\n",
       " 'mean',\n",
       " 'near',\n",
       " 'schools',\n",
       " 'lines',\n",
       " 'table',\n",
       " 'peace',\n",
       " 'modern',\n",
       " 'road',\n",
       " 'red',\n",
       " 'minutes',\n",
       " 'book',\n",
       " 'personal',\n",
       " 'process',\n",
       " 'months',\n",
       " 'situation',\n",
       " 'increase',\n",
       " 'idea',\n",
       " 'english',\n",
       " 'alone',\n",
       " 'women',\n",
       " 'gone',\n",
       " 'living',\n",
       " 'america',\n",
       " 'started',\n",
       " 'longer',\n",
       " 'dr.',\n",
       " 'cut',\n",
       " 'finally',\n",
       " 'third',\n",
       " 'secretary',\n",
       " 'nature',\n",
       " 'private',\n",
       " 'section',\n",
       " 'greater',\n",
       " 'call',\n",
       " 'fire',\n",
       " 'expected',\n",
       " 'needed',\n",
       " 'thats',\n",
       " 'kept',\n",
       " 'ground',\n",
       " 'view',\n",
       " 'values',\n",
       " 'everything',\n",
       " 'pressure',\n",
       " 'dark',\n",
       " 'basis',\n",
       " 'space',\n",
       " 'east',\n",
       " 'father',\n",
       " 'required',\n",
       " 'union',\n",
       " 'spirit',\n",
       " 'complete',\n",
       " 'ones',\n",
       " 'except',\n",
       " 'wrote',\n",
       " 'moved',\n",
       " 'support',\n",
       " 'return',\n",
       " 'conditions',\n",
       " 'recent',\n",
       " 'attention',\n",
       " 'late',\n",
       " 'particular',\n",
       " 'live',\n",
       " 'hope',\n",
       " 'costs',\n",
       " 'hours',\n",
       " 'forces',\n",
       " 'else',\n",
       " 'brown',\n",
       " 'taking',\n",
       " 'couldnt',\n",
       " 'beyond',\n",
       " 'stage',\n",
       " 'read',\n",
       " 'report',\n",
       " 'coming',\n",
       " 'person',\n",
       " 'inside',\n",
       " 'dead',\n",
       " 'low',\n",
       " 'material',\n",
       " 'instead',\n",
       " 'lost',\n",
       " 'heart',\n",
       " 'looking',\n",
       " 'miles',\n",
       " 'data',\n",
       " 'added',\n",
       " 'pay',\n",
       " 'amount',\n",
       " 'followed',\n",
       " 'feeling',\n",
       " '1960',\n",
       " 'single',\n",
       " 'makes',\n",
       " 'research',\n",
       " 'including',\n",
       " 'basic',\n",
       " 'hundred',\n",
       " 'move',\n",
       " 'industry',\n",
       " 'cold',\n",
       " 'simply',\n",
       " '10',\n",
       " 'cant',\n",
       " 'developed',\n",
       " 'tried',\n",
       " 'hold',\n",
       " 'reached',\n",
       " 'committee',\n",
       " 'island',\n",
       " 'defense',\n",
       " 'equipment',\n",
       " 'actually',\n",
       " 'shown',\n",
       " 'son',\n",
       " 'central',\n",
       " 'religious',\n",
       " 'friends',\n",
       " 'river',\n",
       " 'getting',\n",
       " 'st.',\n",
       " 'beginning',\n",
       " 'sort',\n",
       " 'ten',\n",
       " 'received',\n",
       " 'terms',\n",
       " 'trying',\n",
       " 'rest',\n",
       " 'medical']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create equal-sized vector representations of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(words):\n",
    "        \n",
    "    rep = []\n",
    "    \n",
    "    #Get frequency \n",
    "    freq_dict = dict(nltk.FreqDist(words))\n",
    "    \n",
    "    #For words in top 1000 \n",
    "    for word in top_words:\n",
    "        \n",
    "        #If this review has that word\n",
    "        if word in words:\n",
    "            \n",
    "            #Append the tf_idf SCORE\n",
    "            rep.append(idf(word)*freq_dict[word])\n",
    "        else:\n",
    "            \n",
    "            #Otherwise append 0 \n",
    "            rep.append(0)\n",
    "    \n",
    "    #Return answer \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_example = nltk.word_tokenize('fulton county grand jury said friday investigation atlantas recent primary election produced evidence irregularities took place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['perfectly',\n",
       " 'second',\n",
       " 'plenty',\n",
       " 'remarkable',\n",
       " 'psychological',\n",
       " 'try',\n",
       " 'deep',\n",
       " 'neither',\n",
       " 'quality']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1.4609314442700039,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1.7497144369113042,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2.110433458347752,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2.4104569773575526,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2.2706944644322284,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf(words_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Interested in Word Embeddings? Usually used for translation, dialogue generation etc. Consider: \n",
    "#Word2Vec - gensim, Glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> A Simple Application - Sentiment Analysis </h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tf_idf_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18973</th>\n",
       "      <td>18974</td>\n",
       "      <td>833</td>\n",
       "      <td>injects freshness and spirit into the romantic...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96469</th>\n",
       "      <td>96470</td>\n",
       "      <td>5040</td>\n",
       "      <td>to make movies</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>940</td>\n",
       "      <td>34</td>\n",
       "      <td>cliches</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106354</th>\n",
       "      <td>106355</td>\n",
       "      <td>5615</td>\n",
       "      <td>, tone and pace</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55096</th>\n",
       "      <td>55097</td>\n",
       "      <td>2747</td>\n",
       "      <td>unspeakably</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91479</th>\n",
       "      <td>91480</td>\n",
       "      <td>4759</td>\n",
       "      <td>there 's a certain style and wit to the dialogue</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102434</th>\n",
       "      <td>102435</td>\n",
       "      <td>5391</td>\n",
       "      <td>a movie for teens to laugh , groan and hiss</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 1.3136223969474627, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73729</th>\n",
       "      <td>73730</td>\n",
       "      <td>3770</td>\n",
       "      <td>'ve got to admire ... the intensity with which...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70648</th>\n",
       "      <td>70649</td>\n",
       "      <td>3602</td>\n",
       "      <td>is awe and affection -- and a strange urge to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22106</th>\n",
       "      <td>22107</td>\n",
       "      <td>991</td>\n",
       "      <td>treads old turf</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "18973      18974         833   \n",
       "96469      96470        5040   \n",
       "939          940          34   \n",
       "106354    106355        5615   \n",
       "55096      55097        2747   \n",
       "91479      91480        4759   \n",
       "102434    102435        5391   \n",
       "73729      73730        3770   \n",
       "70648      70649        3602   \n",
       "22106      22107         991   \n",
       "\n",
       "                                                   Phrase  Sentiment  \\\n",
       "18973   injects freshness and spirit into the romantic...          3   \n",
       "96469                                      to make movies          2   \n",
       "939                                               cliches          2   \n",
       "106354                                    , tone and pace          2   \n",
       "55096                                         unspeakably          2   \n",
       "91479    there 's a certain style and wit to the dialogue          3   \n",
       "102434        a movie for teens to laugh , groan and hiss          2   \n",
       "73729   've got to admire ... the intensity with which...          3   \n",
       "70648   is awe and affection -- and a strange urge to ...          2   \n",
       "22106                                     treads old turf          1   \n",
       "\n",
       "                                               tf_idf_rep  \n",
       "18973   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "96469   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "939     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "106354  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "55096   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "91479   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "102434  [0, 0, 0, 0, 1.3136223969474627, 0, 0, 0, 0, 0...  \n",
       "73729   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "70648   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "22106   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = df['Phrase'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Skipping earlier pre-processing steps here, but make sure you do them!\n",
    "total_num_sentences = len(sentences)\n",
    "\n",
    "#Calculate idf for word\n",
    "def idf(word):\n",
    "    num_sentences_word_found = 0\n",
    "    for sentence in sentences:\n",
    "        if word in sentence:\n",
    "            num_sentences_word_found +=1 \n",
    "    \n",
    "    if(num_sentences_word_found == 0):\n",
    "        return 1\n",
    "    \n",
    "    #Return idf\n",
    "    return math.log10(total_num_sentences/num_sentences_word_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_corpus = ' '.join(sentences)\n",
    "full_corpus = full_corpus.lower()\n",
    "words = nltk.word_tokenize(full_corpus)\n",
    "final_tokens = []\n",
    "for w in words: \n",
    "    if w not in stop_words: \n",
    "        if w != '':\n",
    "            final_tokens.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words = nltk.FreqDist(final_tokens).most_common(500)\n",
    "top_words = [x for (x,y) in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save all IDF scores for top 500 words\n",
    "idf_dict = {}\n",
    "for word in top_words:\n",
    "    idf_dict[word] = idf(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create tfidf representation\n",
    "def rep_tfidf(row):\n",
    "    rep = []\n",
    "    \n",
    "    #Get words in review\n",
    "    text = row['Phrase']\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words if w.isalpha()]\n",
    "    \n",
    "    #Frequency of dictionary of current review\n",
    "    freq_dict = dict(nltk.FreqDist(words))\n",
    "    \n",
    "    #For words in top 500\n",
    "    for word in top_words:\n",
    "        \n",
    "        #If this review has that word\n",
    "        if word in words:\n",
    "            \n",
    "            #Append the tf_idf SCORE\n",
    "            rep.append(idf_dict[word]*freq_dict[word])\n",
    "        else:\n",
    "            \n",
    "            #Otherwise append 0 \n",
    "            rep.append(0)\n",
    "    \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91479</th>\n",
       "      <td>91480</td>\n",
       "      <td>4759</td>\n",
       "      <td>there 's a certain style and wit to the dialogue</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102434</th>\n",
       "      <td>102435</td>\n",
       "      <td>5391</td>\n",
       "      <td>a movie for teens to laugh , groan and hiss</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73729</th>\n",
       "      <td>73730</td>\n",
       "      <td>3770</td>\n",
       "      <td>'ve got to admire ... the intensity with which...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70648</th>\n",
       "      <td>70649</td>\n",
       "      <td>3602</td>\n",
       "      <td>is awe and affection -- and a strange urge to ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22106</th>\n",
       "      <td>22107</td>\n",
       "      <td>991</td>\n",
       "      <td>treads old turf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "91479      91480        4759   \n",
       "102434    102435        5391   \n",
       "73729      73730        3770   \n",
       "70648      70649        3602   \n",
       "22106      22107         991   \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "91479    there 's a certain style and wit to the dialogue          3  \n",
       "102434        a movie for teens to laugh , groan and hiss          2  \n",
       "73729   've got to admire ... the intensity with which...          3  \n",
       "70648   is awe and affection -- and a strange urge to ...          2  \n",
       "22106                                     treads old turf          1  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1)\n",
    "df = df.iloc[0:25000]\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    12837\n",
       "3     5241\n",
       "1     4254\n",
       "4     1471\n",
       "0     1197\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['tf_idf_rep'] = df.apply(rep_tfidf, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tf_idf_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139892</th>\n",
       "      <td>139893</td>\n",
       "      <td>7589</td>\n",
       "      <td>its borders</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124470</th>\n",
       "      <td>124471</td>\n",
       "      <td>6687</td>\n",
       "      <td>amazing Spider-Man</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76113</th>\n",
       "      <td>76114</td>\n",
       "      <td>3905</td>\n",
       "      <td>the gags , the characters</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134112</th>\n",
       "      <td>134113</td>\n",
       "      <td>7237</td>\n",
       "      <td>the chasm of knowledge that 's opened between ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48136</th>\n",
       "      <td>48137</td>\n",
       "      <td>2347</td>\n",
       "      <td>main strategic objective</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120774</th>\n",
       "      <td>120775</td>\n",
       "      <td>6462</td>\n",
       "      <td>plays out slowly</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85977</th>\n",
       "      <td>85978</td>\n",
       "      <td>4449</td>\n",
       "      <td>in all , an interesting look at the life of th...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115932</th>\n",
       "      <td>115933</td>\n",
       "      <td>6179</td>\n",
       "      <td>at least interesting</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45163</th>\n",
       "      <td>45164</td>\n",
       "      <td>2194</td>\n",
       "      <td>stands still in more ways that one in Clocksto...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1.317073761987874, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47199</th>\n",
       "      <td>47200</td>\n",
       "      <td>2300</td>\n",
       "      <td>is fascinating , though ,</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "139892    139893        7589   \n",
       "124470    124471        6687   \n",
       "76113      76114        3905   \n",
       "134112    134113        7237   \n",
       "48136      48137        2347   \n",
       "120774    120775        6462   \n",
       "85977      85978        4449   \n",
       "115932    115933        6179   \n",
       "45163      45164        2194   \n",
       "47199      47200        2300   \n",
       "\n",
       "                                                   Phrase  Sentiment  \\\n",
       "139892                                        its borders          2   \n",
       "124470                                 amazing Spider-Man          2   \n",
       "76113                           the gags , the characters          2   \n",
       "134112  the chasm of knowledge that 's opened between ...          2   \n",
       "48136                            main strategic objective          2   \n",
       "120774                                   plays out slowly          2   \n",
       "85977   in all , an interesting look at the life of th...          3   \n",
       "115932                               at least interesting          2   \n",
       "45163   stands still in more ways that one in Clocksto...          1   \n",
       "47199                           is fascinating , though ,          3   \n",
       "\n",
       "                                               tf_idf_rep  \n",
       "139892  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "124470  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "76113   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "134112  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "48136   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "120774  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "85977   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "115932  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "45163   [0, 0, 0, 0, 0, 0, 1.317073761987874, 0, 0, 0,...  \n",
       "47199   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#Try out multiple models! Some work better than others depending on the type of data you're working with \n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Getting X and y ready for classification\n",
    "X = df['tf_idf_rep']\n",
    "y = df['Sentiment']\n",
    "X = [x for x in X]\n",
    "y = [[num] for num in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting model\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_tf_idf(string):\n",
    "    rep = []\n",
    "    \n",
    "    #Get words in review\n",
    "    words = word_tokenize(string)\n",
    "    words = [w.lower() for w in words if w.isalpha()]\n",
    "    \n",
    "    #Frequency of dictionary of current review\n",
    "    freq_dict = dict(nltk.FreqDist(words))\n",
    "    \n",
    "    #For words in top 500\n",
    "    for word in top_words:\n",
    "        \n",
    "        #If this review has that word\n",
    "        if word in words:\n",
    "            \n",
    "            #Append the tf_idf SCORE\n",
    "            rep.append(idf_dict[word]*freq_dict[word])\n",
    "        else:\n",
    "            \n",
    "            #Otherwise append 0 \n",
    "            rep.append(0)\n",
    "    \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_string_1 = 'this movie absolutely sucks, it is is deadly dull'\n",
    "test_string_2 = 'this movie is great, there is a wit to the dialogue'\n",
    "test = [string_to_tf_idf(test_string_1), string_to_tf_idf(test_string_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drawback - Reviews not using any of the top 500 words will be assigned the most likely class - in our case, 2. \n",
    "#To avoid this, increase vector size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> All code available on github.com/arkingupta</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
